{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fb1692e-d132-4a2d-920c-c5f68dc339f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "import requests\n",
    "import os\n",
    "\n",
    "def get_specific_assets(url, xpath_image, xpath_details):\n",
    "    options = Options()\n",
    "    options.headless = True\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "    # Initialize the Chrome driver\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    driver.get(url)\n",
    "    time.sleep(2)  # Wait for the page to load\n",
    "\n",
    "    assets = {}\n",
    "\n",
    "    # Fetch the image element using XPath\n",
    "    try:\n",
    "        image_element = driver.find_element(By.XPATH, xpath_image)\n",
    "        original_image_url = image_element.get_attribute('data-zoom')  # Extract the 'data-zoom' attribute\n",
    "        assets['image_urls'] = [original_image_url]\n",
    "\n",
    "        # Check for additional image URLs\n",
    "        base_url, number = original_image_url.rsplit('_', 1)\n",
    "        number = number.split('-')[0]\n",
    "        i = 2\n",
    "        while True:\n",
    "            new_url = f\"{base_url}_{i}\" + original_image_url[original_image_url.rfind('-'):]\n",
    "            print(new_url)\n",
    "            if check_url_validity(new_url):\n",
    "                assets['image_urls'].append(new_url)\n",
    "                i += 1\n",
    "            else:\n",
    "                break\n",
    "    except Exception as e:\n",
    "        print(f\"Error finding image element: {e}\")\n",
    "        assets['image_urls'] = []\n",
    "    # Fetch the details element using XPath\n",
    "    try:\n",
    "        details_element = driver.find_element(By.XPATH, xpath_details)\n",
    "        dt_elements = details_element.find_elements(By.TAG_NAME, 'dt')\n",
    "        dd_elements = details_element.find_elements(By.TAG_NAME, 'dd')\n",
    "\n",
    "        for dt, dd in zip(dt_elements, dd_elements):\n",
    "            key = dt.text.strip()\n",
    "            value = dd.text.strip()\n",
    "            assets[key] = value\n",
    "    except Exception as e:\n",
    "        print(f\"Error finding details element: {e}\")\n",
    "\n",
    "    driver.quit()\n",
    "    return assets\n",
    "\n",
    "# Function to save DataFrame to CSV\n",
    "def save_to_csv(df, filename):\n",
    "    df.to_csv(filename, index=False)\n",
    "\n",
    "\n",
    "def check_url_validity(url):\n",
    "    \"\"\"Check if a URL is valid and accessible.\"\"\"\n",
    "    try:\n",
    "        response = requests.head(url)\n",
    "        return response.status_code == 200\n",
    "    except requests.RequestException:\n",
    "        return False\n",
    "\n",
    "def download_image(image_url, folder_path, max_retries=3, timeout=20):\n",
    "    \"\"\"Download an image from a URL and save it to a specified folder with retry mechanism.\"\"\"\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "\n",
    "    image_name = image_url.split('/')[-1]\n",
    "    image_path = os.path.join(folder_path, image_name)\n",
    "\n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            response = requests.get(image_url)\n",
    "            if response.status_code == 200:\n",
    "                with open(image_path, 'wb') as file:\n",
    "                    file.write(response.content)\n",
    "                return\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error downloading image {image_url}: {e}\")\n",
    "            time.sleep(timeout)\n",
    "            retries += 1\n",
    "\n",
    "    print(f\"Failed to download image after {max_retries} retries: {image_url}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a300a36-eb9a-41f1-99cb-a6ad963a699d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c193bbdd926645d6a64dca7969386bc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9614 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://cdn.openelections.co.uk/uploads/2021/10/18627_2-726x1024.png\n",
      "https://cdn.openelections.co.uk/uploads/2021/10/18627_3-726x1024.png\n",
      "https://cdn.openelections.co.uk/uploads/2021/10/18628_2-726x1024.png\n",
      "https://cdn.openelections.co.uk/uploads/2021/10/18629_2-726x1024.png\n",
      "https://cdn.openelections.co.uk/uploads/2021/10/18630_2-716x1024.png\n",
      "https://cdn.openelections.co.uk/uploads/2021/10/18630_3-716x1024.png\n"
     ]
    }
   ],
   "source": [
    "# Iterate over the URLs in the DataFrame\n",
    "# Initialize an empty DataFrame or read existing data\n",
    "try:\n",
    "    leaflet_data = pd.read_csv('leaflet_data.csv')\n",
    "except FileNotFoundError:\n",
    "    leaflet_data = pd.DataFrame(columns=['URL', 'image_urls', 'local_images', 'Election:', 'Party:', 'Constituency:', 'Mentions:', 'Issues Covered:'])\n",
    "\n",
    "# Read the CSV file with URLs\n",
    "df = pd.read_csv('valid_urls.csv')\n",
    "\n",
    "# XPath of the specific elements\n",
    "xpath_image = '/html/body/div[3]/div[2]/div/div[1]/div[1]/div/div[1]/ul/li[2]/div'\n",
    "xpath_details = '/html/body/div[3]/div[2]/div/div[1]/div[2]/dl'\n",
    "\n",
    "\n",
    "for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    url = row['URL']\n",
    "    leaflet_number = row['Number']\n",
    "\n",
    "    if url not in leaflet_data['URL'].values:\n",
    "        specific_assets = get_specific_assets(url, xpath_image, xpath_details)\n",
    "        specific_assets['URL'] = url\n",
    "\n",
    "        # Download images and update image_urls with local paths\n",
    "        image_folder = f\"leaflet_images/{leaflet_number}\"\n",
    "        local_image_urls = []\n",
    "        for image_url in specific_assets.get('image_urls', []):\n",
    "            download_image(image_url, image_folder)\n",
    "            local_image_name = image_url.split('/')[-1]\n",
    "            local_image_urls.append(os.path.join(image_folder, local_image_name))\n",
    "        specific_assets['local_images'] = local_image_urls\n",
    "\n",
    "        # Append the new row to the DataFrame\n",
    "        new_row_df = pd.DataFrame([specific_assets])\n",
    "        leaflet_data = pd.concat([leaflet_data, new_row_df], ignore_index=True)\n",
    "        \n",
    "        # Save every 100 URLs\n",
    "        if index % 100 == 0:\n",
    "            save_to_csv(leaflet_data, 'leaflet_data.csv')\n",
    "\n",
    "# Save the final DataFrame\n",
    "save_to_csv(leaflet_data, 'leaflet_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37c60b9-3615-4dfe-8830-beaf9d9fbabd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
